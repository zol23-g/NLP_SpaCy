{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93d107",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598cdd3",
   "metadata": {},
   "source": [
    "## 2. Download NLTK Resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d3a18df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zelalem.wubet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zelalem.wubet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\zelalem.wubet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\zelalem.wubet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zelalem.wubet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a664e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\zelalem.wubet/nltk_data', 'e:\\\\Job\\\\.env\\\\nltk_data', 'e:\\\\Job\\\\.env\\\\share\\\\nltk_data', 'e:\\\\Job\\\\.env\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\zelalem.wubet\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)  # Check where NLTK is looking for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee38413",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd436848",
   "metadata": {},
   "source": [
    "## 3. Define the Preprocessing Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e3ed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase and process with spaCy\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Extract tokens that are:\n",
    "    # - alphabetic (no punctuation/numbers)\n",
    "    # - not stopwords\n",
    "    # - not whitespace\n",
    "    cleaned_tokens = [\n",
    "        token.text for token in doc \n",
    "        if token.is_alpha and not token.is_stop and not token.is_space\n",
    "    ]\n",
    "    \n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99756f1d",
   "metadata": {},
   "source": [
    "## 4. Preprocess Sample Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "504b2a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'fascinating', 'field']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is a fascinating field.\"\n",
    "cleaned_tokens = preprocess(text)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c7506",
   "metadata": {},
   "source": [
    "## Tokenization and N-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d67b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = list(ngrams(cleaned_tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa96ebb",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d955fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "Hawaii GPE\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "doc = nlp(sentence)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b91fe",
   "metadata": {},
   "source": [
    "## Converting Text to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7fe843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Output:\n",
      " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      "TF-IDF Vectorizer Output:\n",
      " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.57735027 0.         0.         0.         0.         0.        ]\n",
      " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
      "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
      " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5628291 ]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I love machine learning.\", \"Natural language processing is a part of AI.\", \"AI is the future.\"]\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "X_count = count_vec.fit_transform(sentences)\n",
    "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
    "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41733c5",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c8ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'machine':\n",
      " [-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204    0.023277\n",
      "  0.1297    -0.2112     0.32876    0.67447    0.10047   -0.30559\n",
      "  0.11213    0.22959   -0.32997    0.1389    -0.57289    2.523\n",
      " -0.32921    0.06045    0.23895    0.1091     0.19358   -0.1765\n",
      "  0.11583    0.63204   -0.13644   -0.24354    0.20061   -0.50244\n",
      "  0.40537   -0.38688    0.73784    0.093937  -0.30643    0.045874\n",
      "  0.097915  -0.082114   0.13082   -0.039022   0.088084  -0.27023\n",
      " -0.077658  -0.0045355  0.18986   -0.063083  -0.138      0.40474\n",
      " -0.16199   -0.10953    0.22923   -0.67634   -0.65763   -0.044595\n",
      " -0.12119    0.071167   0.25993   -0.27052   -0.22474   -0.13818\n",
      "  0.20692    0.87604   -0.35257   -0.1498     0.72804    0.68768\n",
      "  0.19993    0.084733  -0.2234     0.11301    0.29895   -0.090119\n",
      "  0.038172  -0.32912    0.014221  -0.36335    0.5898     0.10467\n",
      "  0.16549    0.47199    0.078939  -0.19985    0.84014   -0.2277\n",
      " -0.22907   -0.26243   -0.32598    1.0146    -0.079235  -0.34248\n",
      "  0.032767   0.49757    0.0047373  0.057762   0.19319    0.10756\n",
      "  0.16938    0.42513   -0.22691    0.095343  -0.094303  -0.3849\n",
      " -0.27853   -0.4554     0.2735    -2.1344     0.040352   0.065232\n",
      "  0.23147    0.13766    0.41638    0.1153     0.61593   -0.012896\n",
      "  0.22778    0.13342    0.12902   -0.0054822 -0.55536    0.56218\n",
      " -0.082096  -0.46214    0.21512   -0.09482   -0.16694   -0.94924\n",
      "  0.08068    0.64041    0.0089012  0.043188  -0.030859   0.02754\n",
      "  0.12671   -0.2175     0.0098018  0.14784    0.37847   -0.32479\n",
      " -0.21623    0.14108    0.069705  -0.18381    0.10896   -0.12666\n",
      " -0.063817  -0.44837   -0.079542  -0.54454    0.32602   -0.089619\n",
      " -0.069878   0.010953  -0.16017   -0.13361    0.37901    0.74596\n",
      " -0.01211   -0.4469     0.16831    0.12325   -0.29108   -0.6984\n",
      " -0.33013   -0.20759   -0.070089   0.023017  -0.32895   -0.02034\n",
      "  0.50521   -0.12432    0.26341   -0.063996  -0.46205   -0.39595\n",
      " -0.15154   -0.22158   -0.050627  -0.015164  -0.38784    0.5011\n",
      "  0.19628   -0.31646   -0.48555   -0.49464    0.35255   -0.060035\n",
      "  0.082212   0.084107   0.17729   -0.55179    0.071874  -0.39032\n",
      "  0.40137   -0.2273     0.35788    0.42503   -0.20496    0.58632\n",
      " -0.2015     0.35892    0.045149  -0.20252    0.15502   -0.019122\n",
      " -0.11768   -0.48471    0.35088    0.14332    0.091038   0.28448\n",
      "  0.35166   -0.87305    0.047971   0.43431   -0.34814    0.035735\n",
      " -0.21673    0.062818  -0.40837   -0.21775    0.1597     0.56172\n",
      " -0.11126    0.33851   -0.31825   -0.10671    0.057792  -0.03997\n",
      " -0.15047   -0.11435    0.56779    0.43056   -0.17674   -0.045657\n",
      "  0.04453   -0.11629    0.094277  -0.46008   -0.12373   -0.18918\n",
      "  0.14565   -0.17643    0.45186   -0.011373   0.16214   -0.17391\n",
      " -0.14195    0.14683   -0.055814   0.52315    0.0032239 -0.51676\n",
      " -0.094159   0.21092    0.22586   -0.78028   -0.67057   -0.031255\n",
      "  0.045657   0.033926   0.16709    0.014854  -0.14456   -0.56059\n",
      " -0.26709   -0.17691    0.15472    0.46348    1.4026    -0.24662\n",
      "  0.3447    -0.56387    0.047655  -0.39049   -0.016389   0.52322\n",
      " -0.22908   -0.31134    0.6579    -0.67904    0.40002    0.055284\n",
      "  0.5956     0.031032  -0.19315   -0.65318   -0.28911    0.13658\n",
      " -0.42426   -0.37742   -0.17644   -0.15885    0.48907    1.0195\n",
      "  0.12087   -0.36731    0.0087637  0.10666   -0.12636    0.66767  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "word = nlp(\"machine\")[0]\n",
    "print(\"Vector for 'machine':\\n\", word.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b3398",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
